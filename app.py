import streamlit as st
import os
import json  
from dotenv import load_dotenv

from llama_index.core import (
    StorageContext,
    Settings,
    load_index_from_storage,
    PromptTemplate
)
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.response_synthesizers import get_response_synthesizer
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.openai import OpenAI


load_dotenv()
PERSIST_DIR = "./storage"
EMBED_MODEL_NAME = "neuralmind/bert-base-portuguese-cased"
HISTORY_FILE = "history.json" 

st.set_page_config(
    page_title="An√°lise de Ementas vs S√∫mulas do STJ",
    page_icon="‚öñÔ∏è",
    layout="wide"
)

def load_history():
    if os.path.exists(HISTORY_FILE):
        with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
            try:
                return json.load(f)
            except json.JSONDecodeError:
                return [] 
    return []

def save_history():
    with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
        json.dump(st.session_state.history, f, indent=4)

if "page" not in st.session_state:
    st.session_state.page = "home"

if "history" not in st.session_state:
    st.session_state.history = load_history()


@st.cache_resource(show_spinner="Carregando base de conhecimento das S√∫mulas...")
def load_rag_system():
    if not os.path.exists(PERSIST_DIR):
        return None
    Settings.embed_model = HuggingFaceEmbedding(model_name=EMBED_MODEL_NAME)
    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)
    index = load_index_from_storage(storage_context)
    return index


def show_home():
    st.title("‚öñÔ∏è Analisador de Ementas vs S√∫mulas do STJ")
    st.markdown("""
        ### Bem-vindo!
        Esta aplica√ß√£o utiliza um sistema de Intelig√™ncia Artificial (RAG) para verificar se uma ementa jur√≠dica
        est√° em conson√¢ncia com alguma das **S√∫mulas do Superior Tribunal de Justi√ßa (STJ)**.
    """)

    st.markdown("---")

    _, col2, _ = st.columns([2, 3, 2])
    with col2:
        if st.button("üöÄ Iniciar An√°lise", use_container_width=True, type="primary"):
            st.session_state.page = "analysis"
            st.rerun()

def show_analysis():
    st.title("‚öñÔ∏è Assistente Jur√≠dico STJ")

    index = load_rag_system()

    if not index:
        st.error(f"Diret√≥rio do √≠ndice '{PERSIST_DIR}' n√£o encontrado. Por favor, execute o script 'build.py' primeiro para criar a base de dados.")
        st.stop()

    if not os.getenv("OPENAI_API_KEY"):
        st.error("A chave da API da OpenAI (OPENAI_API_KEY) n√£o foi encontrada. Por favor, configure seu arquivo .env.")
        st.stop()

    with st.sidebar:
        st.header("‚ÑπÔ∏è Instru√ß√µes")
        st.markdown(
            """
            1. **Cole a ementa** na √°rea de texto principal.
            2. Clique em **Analisar Ementa**.
            3. A IA ir√° extrair a ementa, buscar s√∫mulas relevantes e gerar uma an√°lise completa.
            """
        )

        st.header("üìÇ Hist√≥rico de Consultas")
        if st.button("üóëÔ∏è Limpar Hist√≥rico"):
            st.session_state.history = []
            save_history()
            st.rerun()

        if st.session_state.history:
            for i, item in enumerate(reversed(st.session_state.history)):
                with st.expander(f"Consulta {len(st.session_state.history) - i}"):
                    st.text_area("Ementa:", value=item["ementa"], height=100, disabled=True, key=f"hist_q_{i}")
                    st.markdown("**Resposta da IA:**")
                    st.markdown(item["resposta"], unsafe_allow_html=True)
        else:
            st.info("Nenhuma consulta foi realizada ainda.")

    user_query = st.text_area(
        "### üìÑ Cole o texto da ementa jur√≠dica aqui:",
        height=250,
        placeholder="AGRAVO REGIMENTAL NO HABEAS CORPUS. TR√ÅFICO DE DROGAS. DOSIMETRIA. PENA-BASE..."
    )

    if st.button("üîç Analisar Ementa", use_container_width=True, type="primary"):
        if not user_query.strip():
            st.warning("‚ö†Ô∏è Por favor, insira o texto da ementa antes de analisar.")
            return

        with st.spinner("ü§ñ Analisando... A IA est√° processando a ementa e buscando as s√∫mulas..."):
            try:
                llm_extrator = OpenAI(
                    model="gpt-3.5-turbo",
                    temperature=0,
                    api_key=os.getenv("OPENAI_API_KEY")
                )
                extracao_prompt_str = (
                    "Sua tarefa √© identificar e extrair APENAS o texto que constitui uma ementa jur√≠dica do texto fornecido. "
                    "Ignore perguntas, instru√ß√µes, sauda√ß√µes ou qualquer texto explicativo. "
                    "Se houver m√∫ltiplos blocos, extraia apenas o que parece ser a ementa oficial.\n\n"
                    "Texto do usu√°rio: {full_query}\n\n"
                    "Ementa jur√≠dica extra√≠da (apenas o texto oficial):"
                )
                extracao_prompt = PromptTemplate(extracao_prompt_str)
                response = llm_extrator.complete(extracao_prompt.format(full_query=user_query))
                ementa_extraida = response.text.strip()

                query_generation_prompts = [
                    "Extraia os 3-4 termos jur√≠dicos mais importantes desta ementa: {ementa_text}",
                    "Qual √© a tese jur√≠dica principal desta ementa em uma frase de at√© 15 palavras? {ementa_text}",
                    "Qual o principal instituto jur√≠dico tratado nesta ementa? {ementa_text}"
                ]
                queries_otimizadas = [llm_extrator.complete(p.format(ementa_text=ementa_extraida)).text.strip() for p in query_generation_prompts]

                retriever = index.as_retriever(similarity_top_k=8)
                all_retrieved_nodes = []
                retrieved_ids = set()

                for query in queries_otimizadas:
                    retrieved_nodes = retriever.retrieve(query)
                    for node_with_score in retrieved_nodes:
                        node_id = node_with_score.node.metadata.get('id', node_with_score.node.node_id)
                        if node_id not in retrieved_ids:
                            all_retrieved_nodes.append(node_with_score)
                            retrieved_ids.add(node_id)

                all_retrieved_nodes.sort(key=lambda x: x.score, reverse=True)
                final_retrieved_nodes = all_retrieved_nodes[:10]

                Settings.llm = OpenAI(model="gpt-4-turbo", api_key=os.getenv("OPENAI_API_KEY"))

                qa_prompt_tmpl_str = (
                    "Voc√™ √© um especialista em S√∫mulas do STJ e deve fazer uma an√°lise precisa.\n"
                    "TAREFA: Analisar se a ementa apresentada est√° em conson√¢ncia com alguma das S√∫mulas do STJ fornecidas.\n\n"
                    "S√öMULAS RELEVANTES ENCONTRADAS:\n---------------------\n{context_str}\n---------------------\n\n"
                    "EMENTA PARA AN√ÅLISE:\n---------------------\n{query_str}\n---------------------\n\n"
                    "INSTRU√á√ïES PARA RESPOSTA:\n"
                    "1. Se encontrar conson√¢ncia: Cite o n√∫mero da S√∫mula, transcreva seu texto completo e explique detalhadamente a rela√ß√£o.\n"
                    "2. Se n√£o encontrar: Afirme claramente que n√£o h√° conson√¢ncia direta e, se poss√≠vel, mencione brevemente os temas das S√∫mulas encontradas para mostrar que foram analisadas.\n"
                    "3. Seja claro, objetivo e use formata√ß√£o markdown para organizar a resposta."
                )
                qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)

                response_synthesizer = get_response_synthesizer(
                    response_mode="compact",
                    text_qa_template=qa_prompt_tmpl,
                )

                class CustomRetriever:
                    def __init__(self, nodes): self.nodes = nodes
                    def retrieve(self, query): return self.nodes

                query_engine = RetrieverQueryEngine(
                    retriever=CustomRetriever(final_retrieved_nodes),
                    response_synthesizer=response_synthesizer,
                )

                final_response = query_engine.query(ementa_extraida)

                st.success("‚úÖ An√°lise Conclu√≠da!")

                st.markdown("### üß† Resposta do Assistente Jur√≠dico:")
                st.markdown(str(final_response))

                st.session_state.history.append({
                    "ementa": user_query,
                    "resposta": str(final_response)
                })
                save_history()

                with st.expander("üîç Ver detalhes do processo de an√°lise da IA"):
                    st.text_area("1. Ementa Extra√≠da para An√°lise:", value=ementa_extraida, height=150, disabled=True)

                    st.write("2. Consultas Geradas para a Busca:")
                    for i, query in enumerate(queries_otimizadas):
                        st.text_input(f"Consulta {i+1}:", value=query, disabled=True, key=f"query_{i}")

                    st.write(f"3. S√∫mulas Encontradas ({len(final_retrieved_nodes)} resultados):")
                    for node_com_score in final_retrieved_nodes:
                        sumula_id = node_com_score.node.metadata.get('id', 'N/A')
                        st.markdown(f"**S√∫mula {sumula_id}** | Similaridade: `{node_com_score.score:.4f}`")
                        st.text_area(
                            label=f"Conte√∫do S√∫mula {sumula_id}", value=node_com_score.node.get_content(),
                            height=80, disabled=True, key=f"content_{node_com_score.node.node_id}",
                            label_visibility="collapsed"
                        )

            except Exception as e:
                st.error(f"‚ùå Ocorreu um erro durante a an√°lise: {e}")

if st.session_state.page == "home":
    show_home()
else:
    show_analysis()